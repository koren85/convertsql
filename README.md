# SQL Converter: MS SQL to PostgreSQL

Этот проект предназначен для автоматической конвертации скриптов MS SQL в PostgreSQL с проверкой синтаксиса и выполнения в PostgreSQL. Решение автоматизирует процесс миграции большого количества скриптов, обеспечивая адаптацию синтаксиса и проверку работоспособности.

## Особенности

- Конвертация синтаксиса MS SQL в PostgreSQL
- Подстановка тестовых значений параметров
- Проверка синтаксиса и выполнения в PostgreSQL через Docker
- Автоматические попытки исправления ошибок
- **Интеграция с нейросетями для сложных конверсий**
- Параллельная обработка для ускорения процесса
- Подробное логирование процесса конвертации
- Генерация отчетов в формате HTML, Excel и JSON
- Поддержка пакетной обработки через YAML-конфигурации

## Требования

- Python 3.7+
- Docker
- API ключи для нейросетей (опционально)
- Все библиотеки из requirements.txt

## Установка

1. Клонируйте репозиторий:

```bash
git clone <репозиторий>
cd convertsql
```

2. Запустите скрипт настройки, который создаст виртуальное окружение и установит зависимости:

```bash
python setup.py
```

3. Активируйте виртуальное окружение:

```bash
# для Linux/MacOS:
source venv/bin/activate
# для Windows:
venv\Scripts\activate
```

4. Запустите PostgreSQL в Docker:

```bash
docker-compose up -d
```

5. Настройте файл с переменными окружения:

```bash
# Скопируйте шаблон .env файла
cp .env.example .env

# Отредактируйте .env файл и добавьте ваши API ключи
nano .env   # или любой другой редактор
```

## Использование

### Конвертация отдельного скрипта (тестирование)

Для тестирования конвертации одного скрипта:

```bash
python test_converter.py scripts/examples/example1.sql
```

Это выведет подробную информацию о процессе конвертации с показом оригинального и сконвертированного скриптов, а также результатов тестирования в PostgreSQL.

Для сохранения результата добавьте опцию `--save`:

```bash
python test_converter.py scripts/examples/example1.sql --save converted/example1.sql
```

### Конвертация с использованием нейросети

Для тестирования конвертации с использованием нейросети:

```bash
python test_ai_converter.py scripts/examples/complex_example.sql
```

Вы можете выбрать провайдера нейросети:

```bash
python test_ai_converter.py scripts/examples/complex_example.sql --provider openai
python test_ai_converter.py scripts/examples/complex_example.sql --provider anthropic
```

Также вы можете указать конкретный .env файл:

```bash
python test_ai_converter.py scripts/examples/complex_example.sql --env path/to/custom.env
```

### Конвертация всех скриптов из директории

Для конвертации всех SQL-скриптов из директории:

```bash
python main.py scripts/examples
```

Дополнительные опции:

```bash
python main.py scripts/examples --output converted/output --parallel 8 --report --max-retry 5 --use-ai
```

Где:
- `--output` - директория для сохранения сконвертированных скриптов
- `--parallel` - количество параллельных потоков (по умолчанию 4)
- `--report` - генерировать отчет после завершения
- `--max-retry` - максимальное количество попыток исправления ошибок (по умолчанию 3)
- `--use-ai` - использовать нейросеть для сложных случаев
- `--no-ai` - не использовать нейросеть даже если она включена в конфигурации
- `--ai-provider` - указать провайдера нейросети ('openai' или 'anthropic')
- `--env` - путь к конкретному .env файлу

### Проверка конвертации примеров

Для проверки работы конвертера на примерах из директории:

```bash
python check_examples.py --verbose
```

Это выполнит все примеры из директории `scripts/examples` и выведет подробную информацию о результатах.

Для сохранения сконвертированных скриптов:

```bash
python check_examples.py --output converted/examples
```

### Детальный анализ скрипта

Для детального анализа SQL-скрипта и выявления возможных проблем при конвертации:

```bash
python analyze_script.py scripts/examples/complex_example.sql
```

Это выведет информацию о структуре скрипта, использованных параметрах, функциях и потенциальных проблемах.

Для сохранения отчета в JSON:

```bash
python analyze_script.py scripts/examples/complex_example.sql --output reports/analysis.json
```

### Пакетная обработка с конфигурацией

Для выполнения пакетной обработки с использованием YAML-конфигурации:

```bash
python batch_process.py configs/example_batch.yaml --verbose
```

Пример конфигурации:

```yaml
# Пример конфигурации для пакетной обработки
name: example_batch
description: "Пример пакетной обработки примеров SQL-скриптов"

# Пути к директориям
input_dir: "scripts/examples"
output_dir: "converted/examples"

# Настройки обработки
retry_count: 3
parallel: 4
generate_html_report: true
use_ai: true  # Использовать нейросеть
ai_provider: "openai"  # Провайдер нейросети

# Пользовательские параметры для подстановки
params:
  startDate: "'2023-01-01'"
  endDate: "'2023-12-31'"
  userTypeID: "1"
  region: "'Europe'"
  topRank: "10"
  categoryID: "5"
  minSalesAmount: "1000"
```

## Структура проекта

```
convertsql/
├── config.py               # Конфигурации и настройки
├── main.py                 # Основной скрипт запуска
├── test_converter.py       # Тестирование конвертации одного скрипта
├── test_ai_converter.py    # Тестирование конвертации с нейросетью
├── check_examples.py       # Проверка конвертации примеров
├── analyze_script.py       # Детальный анализ скрипта
├── batch_process.py        # Пакетная обработка с конфигурацией
├── setup.py                # Настройка окружения
├── requirements.txt        # Зависимости
├── docker-compose.yml      # Docker-конфигурация
├── .env.example            # Шаблон файла с переменными окружения
├── .env                    # Файл с переменными окружения (не включен в репозиторий)
├── configs/                # YAML-конфигурации для пакетов
├── scripts/                # Исходные скрипты
│   └── examples/           # Примеры скриптов
├── converted/              # Сконвертированные скрипты
├── logs/                   # Логи конвертации
├── reports/                # Отчеты
└── src/
    ├── __init__.py
    ├── parser.py           # Парсер скриптов
    ├── converter.py        # Конвертер синтаксиса
    ├── postgres_tester.py  # Тестирование в PostgreSQL
    ├── ai_converter.py     # Конвертация с использованием нейросетей
    ├── logger.py           # Логирование
    └── report_generator.py # Генерация отчетов
```

## Настройка .env файла

Для настройки параметров и API ключей используется файл `.env`. Пример содержимого:

```
# API ключи для нейросетей
OPENAI_API_KEY=sk-ваш_openai_ключ
ANTHROPIC_API_KEY=sk-ant-ваш_anthropic_ключ

# Настройки для PostgreSQL
PG_HOST=localhost
PG_PORT=5432
PG_DATABASE=testdb
PG_USER=testuser
PG_PASSWORD=testpassword

# Настройки использования нейросетей
USE_AI_CONVERSION=true
AI_PROVIDER=openai  # или anthropic

# Модели нейросетей
OPENAI_MODEL=gpt-4  # или gpt-3.5-turbo для более быстрой работы
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Параметры запросов к нейросетям
AI_TEMPERATURE=0.1
AI_MAX_TOKENS=4000
AI_RETRY_COUNT=2
AI_FALLBACK_THRESHOLD=2
```

Вы можете создать несколько разных `.env` файлов для различных конфигураций и указывать нужный при запуске с помощью параметра `--env`.

## Логирование и отчеты

### Логирование

В процессе работы конвертера создаются два типа логов:
1. Общий лог в формате текста в директории `logs/`
2. Детальные логи по каждому скрипту в формате JSON в этой же директории

### Отчеты

После завершения конвертации с опцией `--report` создаются следующие отчеты:
1. HTML-отчет с общей статистикой и статусом каждого скрипта
2. Excel-отчет с подробной информацией
3. JSON-отчет со всеми деталями процесса конвертации

## Настройка и расширение

### Настройка конвертера

Основные настройки конвертера можно изменить в файле `config.py` или в `.env` файле:
- Параметры подключения к PostgreSQL
- Маппинг типов данных MS SQL и PostgreSQL
- Маппинг функций
- Значения параметров по умолчанию
- Настройки нейросетей
- Максимальное время выполнения скрипта

### Настройка использования нейросетей

В файле `.env` можно настроить параметры использования нейросетей:

```
# Настройки использования нейросетей
USE_AI_CONVERSION=true
AI_PROVIDER=openai  # или anthropic

# Модели нейросетей
OPENAI_MODEL=gpt-4  # или gpt-3.5-turbo для более быстрой работы
ANTHROPIC_MODEL=claude-3-sonnet-20240229

# Параметры запросов к нейросетям
AI_TEMPERATURE=0.1
AI_MAX_TOKENS=4000
AI_RETRY_COUNT=2
AI_FALLBACK_THRESHOLD=2
```

### Добавление поддержки новых конструкций

Для добавления поддержки новых конструкций MS SQL необходимо:
1. Добавить соответствующие записи в маппинги в `config.py`
2. Реализовать обработку конструкций в `src/converter.py`

### Расширение функций исправления ошибок

Механизм автоматического исправления ошибок расположен в методе `fix_script()` класса `PostgresTester` в файле `src/postgres_tester.py`. Для добавления новых правил исправления ошибок необходимо расширить метод `_try_standard_fixes()`.

### Добавление поддержки новых провайдеров нейросетей

Для добавления поддержки новых провайдеров нейросетей необходимо:
1. Добавить новый метод в класс `AIConverter` в файле `src/ai_converter.py`
2. Добавить обработку нового провайдера в метод `convert_with_ai()`

## Примеры параметров и их подстановка

Параметры в исходных скриптах должны быть указаны в формате `{имя_параметра}`. Например:

```sql
SELECT * FROM users WHERE created_date BETWEEN {startDate} AND {endDate}
```

Эти параметры будут автоматически заменены на значения из `DEFAULT_PARAMS` в `config.py` или из пользовательских параметров, указанных в YAML-конфигурации для пакетной обработки.

## Использование нейросетей

### Принцип работы

1. Сначала конвертер пытается преобразовать скрипт стандартными методами.
2. Если стандартная конвертация не удается после нескольких попыток (определяется параметром `AI_FALLBACK_THRESHOLD`), конвертер обращается к нейросети.
3. Нейросеть получает исходный скрипт MS SQL и сообщение об ошибке из PostgreSQL, а затем пытается создать корректный PostgreSQL скрипт.
4. Результат от нейросети тестируется в PostgreSQL и, если успешен, используется как окончательный вариант конвертации.

### Преимущества использования нейросетей

- Обработка сложных случаев, которые не покрываются стандартными правилами конвертации
- Адаптация под специфические особенности конкретных скриптов
- Возможность конвертации нестандартных конструкций и специфичного синтаксиса
- Исправление ошибок с учетом контекста всего скрипта

### Ограничения

- Требуется API ключ для доступа к нейросетям
- Может быть медленнее стандартной конвертации
- Зависит от качества работы API нейросети
- Может потребовать оплаты при большом объеме конвертаций

## Безопасность

- Файл `.env` с API ключами добавлен в `.gitignore` и не должен включаться в репозиторий
- Доступ к API нейросетей должен быть защищен и ограничен
- Конфиденциальные данные не должны передаваться в скриптах

## Ограничения

- Конвертер не поддерживает сложную логику хранимых процедур (T-SQL)
- Некоторые специфические конструкции MS SQL могут требовать ручной доработки
- Временные таблицы в формате `@table_name` требуют особой обработки
- При использовании нейросетей следует учитывать ограничения по использованию API

## Вклад в проект

Мы приветствуем вклад в развитие проекта! Если вы хотите добавить новую функциональность или исправить ошибки, пожалуйста, создайте Pull Request.

## Лицензия

Этот проект распространяется под лицензией MIT.

## Быстрый старт: одиночная конвертация через нейросеть

Для конвертации одного скрипта с помощью нейросети:

```bash
python test_ai_converter.py scripts/examples/example2.sql --provider anthropic --skip-docker-check --save converted/anthropic_result2.sql
```

- `--provider` — выбрать провайдера нейросети (`anthropic` или `openai`)
- `--skip-docker-check` — не проверять Docker
- `--save` — путь для сохранения результата

Можно также указать дополнительные параметры (см. `--help`).

## Пакетная обработка скриптов (batch mode)

Для пакетной обработки всех скриптов из директории по YAML-конфигу:

```bash
python batch_process.py configs/config.yaml --skip-docker-check --verbose
```

- `configs/config.yaml` — путь к вашему YAML-конфигу (пример ниже)
- `--skip-docker-check` — не проверять Docker
- `--verbose` — подробный вывод
- `--provider anthropic` — явно указать провайдера (перекроет значение из yaml)
- `--max-iterations 3` — максимальное число итераций AI-конвертации (перекроет значение из yaml)

**После завершения обработки:**
- Все сконвертированные скрипты будут в директории, указанной в `output_dir`.
- Итоговый отчёт (HTML/Excel/JSON) будет сгенерирован в директории `reports/`.
- В отчёте будут все скрипты: успешные и с ошибками, с текстом ошибки (если была).

### Пример актуального YAML-конфига

```yaml
name: example_batch
input_dir: "scripts/examples"
output_dir: "converted/examples"
retry_count: 3
parallel: 4
generate_html_report: true
ai_provider: anthropic
max_iterations: 3
params:
  startDate: "'2023-01-01'"
  endDate: "'2023-12-31'"
  userTypeID: "1"
  region: "'Europe'"
  topRank: "10"
  categoryID: "5"
  minSalesAmount: "1000"
```

- `ai_provider` и `max_iterations` можно не указывать, если задаёте их через CLI.
- `params` — параметры для подстановки в скрипты.

## Примеры команд

**Одиночная конвертация через нейросеть:**
```bash
python test_ai_converter.py scripts/examples/example2.sql --provider anthropic --skip-docker-check --save converted/anthropic_result2.sql
```

**Пакетная обработка:**
```bash
python batch_process.py configs/config.yaml --skip-docker-check --verbose
```

**Пакетная обработка с явным указанием провайдера и числа итераций:**
```bash
python batch_process.py configs/config.yaml --provider openai --max-iterations 5 --skip-docker-check --verbose
```
